{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syncing from google drive and github... for more info on this code, refer [here](https://zerowithdot.com/colab-github-workflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from os.path import join\n",
    "\n",
    "ROOT = '/content/drive'     # default for the drive\n",
    "drive.mount(ROOT)           # we mount the drive at /content/drive\n",
    "\n",
    "GIT_PATH = \"https://github.com/ybchen97/filler_detection.git\"\n",
    "!git clone \"{GIT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages in this local notebook specified in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r '/content/filler_detection/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and setting up env variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "import wave\n",
    "import pylab\n",
    "from tf_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Import files for trigger-word detection model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO = \"./filler_detection/\"\n",
    "POSITIVE_DIRECTORY = \"raw_data/positive_data/\"\n",
    "BACKGROUND_DIRECTORY = \"raw_data/background_data/\"\n",
    "NEGATIVES_DIRECTORY = \"raw_data/google_dataset/\"\n",
    "NEGATIVES_TRUNCATED_DIRECTORY = \"raw_data/google_dataset_truncated/\"\n",
    "AUDIO_EXAMPLES_DIRECTORY = \"audio_examples/\"\n",
    "POSITIVE_EXAMPLE = \"jh_1.wav\"\n",
    "AUDIO_EXAMPLE = \"example_train.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f\"{REPO}trained_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Audio into 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    print(\"PREPROCESSING...\")\n",
    "    # Trim or pad audio segment to 10000ms\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment)\n",
    "    # Set frame rate to 123000\n",
    "    segment = segment.set_channels(1)\n",
    "    segment = segment.set_frame_rate(123000)\n",
    "    # Export as wav\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filler word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    \"\"\"\n",
    "    Function to take filename and generate a prediction vector.\n",
    "    \n",
    "    Argument:\n",
    "    filename -- Audio file to run prediction on\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- Prediction vector with probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_rate, samples = wavfile.read(filename)\n",
    "    _, _, x = signal.spectrogram(samples, sample_rate)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # the spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    predictions = model.predict(x)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_filler_word(filename, threshold):\n",
    "    \"\"\"\n",
    "    Function to count the number of times trigger word spoken in audio.\n",
    "    \n",
    "    Arguments:\n",
    "    filename -- Audio file to run prediction on\n",
    "    threshold -- Probability above which trigger word considered present\n",
    "    \"\"\"\n",
    "\n",
    "    preprocess_audio(filename)\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(CHIME_FILE)\n",
    "    predictions = detect_triggerword(filename)\n",
    "    Ty = predictions.shape[1]\n",
    "    \n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter filename and probability threshold for determining filler word\n",
    "count_filler_word('./Test_2.wav', 0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
