{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syncing from google drive and github... for more info on this code, refer [here](https://zerowithdot.com/colab-github-workflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from os.path import join\n",
    "\n",
    "ROOT = '/content/drive'     # default for the drive\n",
    "drive.mount(ROOT)           # we mount the drive at /content/drive\n",
    "\n",
    "GIT_PATH = \"https://github.com/ybchen97/filler_detection.git\"\n",
    "!git clone \"{GIT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages in this local notebook specified in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r '/content/filler_detection/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and setting up env variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "import wave\n",
    "import pylab\n",
    "from tf_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Import files for trigger-word detection model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MACHINE_DIRECTORY = \"./\"\n",
    "COLAB_DIRECTORY = \"./filler-detection\"\n",
    "REPO_DIRECTORY = LOCAL_MACHINE_DIRECTORY # set this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_DIRECTORY = \"raw_data/positive_data/\"\n",
    "BACKGROUND_DIRECTORY = \"raw_data/background_data/\"\n",
    "NEGATIVES_DIRECTORY = \"raw_data/google_dataset/\"\n",
    "NEGATIVES_TRUNCATED_DIRECTORY = \"raw_data/google_dataset_truncated/\"\n",
    "AUDIO_EXAMPLES_DIRECTORY = \"audio_examples/\"\n",
    "POSITIVE_EXAMPLE = \"jh_1.wav\"\n",
    "AUDIO_EXAMPLE = \"example_train.wav\"\n",
    "CHIME_FILE = \"audio_examples/chime.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5490, 129)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1369, 196)         379456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1369, 196)         784       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1369, 196)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1369, 196)         0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 1369, 128)         124800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1369, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1369, 128)         512       \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 1369, 128)         98688     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1369, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1369, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1369, 128)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1369, 1)           129       \n",
      "=================================================================\n",
      "Total params: 604,881\n",
      "Trainable params: 603,977\n",
      "Non-trainable params: 904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for local machine\n",
    "model = load_model(\"./new_trained_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5490, 129)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1369, 196)         379456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1369, 196)         784       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1369, 196)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1369, 196)         0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 1369, 128)         124800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1369, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1369, 128)         512       \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 1369, 128)         98688     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1369, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1369, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1369, 128)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1369, 1)           129       \n",
      "=================================================================\n",
      "Total params: 604,881\n",
      "Trainable params: 603,977\n",
      "Non-trainable params: 904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for colab\n",
    "model = load_model(f\"{REPO_DIRECTORY}/new_trained_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Audio into 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    print(\"PREPROCESSING...\")\n",
    "    # Trim or pad audio segment to 10000ms\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment)\n",
    "    # Set frame rate to 123000\n",
    "    segment = segment.set_channels(1)\n",
    "    segment = segment.set_frame_rate(123000)\n",
    "    # Export as wav\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filler word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    \"\"\"\n",
    "    Function to take filename and generate a prediction vector.\n",
    "    \n",
    "    Argument:\n",
    "    filename -- Audio file to run prediction on\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- Prediction vector with probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_rate, samples = wavfile.read(filename)\n",
    "    _, _, x = signal.spectrogram(samples, sample_rate)\n",
    "    print(x.shape)\n",
    "    \n",
    "    # the spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    predictions = model.predict(x)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_filler_word(filename, threshold):\n",
    "    \"\"\"\n",
    "    Function to count the number of times trigger word spoken in audio.\n",
    "    \n",
    "    Arguments:\n",
    "    filename -- Audio file to run prediction on\n",
    "    threshold -- Probability above which trigger word considered present\n",
    "    \"\"\"\n",
    "\n",
    "    preprocess_audio(filename)\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(CHIME_FILE)\n",
    "    predictions = detect_triggerword(filename)\n",
    "    Ty = predictions.shape[1]\n",
    "    \n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CHIME_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-efe4aeb467fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enter filename and probability threshold for determining filler word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_filler_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./audio_ignored_examples/train_10.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8473d66a2a6a>\u001b[0m in \u001b[0;36mcount_filler_word\u001b[0;34m(filename, threshold)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpreprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0maudio_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mchime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHIME_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_triggerword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mTy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CHIME_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "# Enter filename and probability threshold for determining filler word\n",
    "count_filler_word('./audio_ignored_examples/train_10.wav', 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
