{
<<<<<<< Updated upstream
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "my6I-6tlRSuK"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAbmFvBVRXtY"
   },
   "source": [
    "Syncing from google drive and github... for more info on this code, refer [here](https://zerowithdot.com/colab-github-workflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FO4R_OEE3LSV"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from os.path import join\n",
    "\n",
    "ROOT = '/content/drive'     # default for the drive\n",
    "PROJ = 'My Drive/filler_detection/train_data/XY_Train'       # path to your project on Drive\n",
    "PROJ2= 'My Drive/filler_detection/train_data/dev_npy'\n",
    "\n",
    "drive.mount(ROOT)           # we mount the drive at /content/drive\n",
    "\n",
    "PROJECT_PATH = join(ROOT, PROJ)\n",
    "PROJCT_PATH2 = join(ROOT, PROJ2)\n",
    "!mkdir \"{PROJECT_PATH}\"     # in case we haven't created it already   \n",
    "!mkdir \"{PROJECT_PATH2}\"\n",
    "\n",
    "GIT_PATH = \"https://github.com/ybchen97/filler_detection.git\"\n",
    "!mkdir ./temp\n",
    "!git clone \"{GIT_PATH}\"\n",
    "!mv ./temp/* \"{PROJECT_PATH}\"\n",
    "!rm -rf ./temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa4hVHm5RouU"
   },
   "source": [
    "Install packages in this local notebook specified in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3SSCyXL8lBb"
   },
   "outputs": [],
   "source": [
    "!pip install -r '/content/filler_detection/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7Hn1LtPRxZS"
   },
   "source": [
    "Importing and setting up env variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rasqjnO6mAq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "import wave\n",
    "import pylab\n",
    "from tf_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Import files for trigger-word detection model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shTwr2KwE6MW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from os.path import join\n",
    "\n",
    "ROOT = '/content/drive'     # default for the drive\n",
    "PROJ = 'My Drive/filler_detection/train_data/XY_Train'       # path to your project on Drive\n",
    "\n",
    "DATA_PATH = join(ROOT, PROJ)\n",
    "REPO = \"/content/filler_detection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lgJ6UdH9ViH"
   },
   "outputs": [],
   "source": [
    "POSITIVE_DIRECTORY = \"./raw_data/positive_data/\"\n",
    "BACKGROUND_DIRECTORY = \"./raw_data/background_data/\"\n",
    "NEGATIVES_DIRECTORY = \"./raw_data/google_dataset/\"\n",
    "NEGATIVES_TRUNCATED_DIRECTORY = \"./raw_data/google_dataset_truncated/\"\n",
    "AUDIO_EXAMPLES_DIRECTORY = \"./audio_examples/\"\n",
    "POSITIVE_EXAMPLE = \"jh_1.wav\"\n",
    "AUDIO_EXAMPLE = \"example_train.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eM15MVGR0eJ"
   },
   "source": [
    "The fun begins...\n",
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b54E5A1c8pSh"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras library.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "    \n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    # Step 1: CONV Layer\n",
    "    # CONV-1D\n",
    "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)\n",
    "    # Batch Normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    # RelU activation\n",
    "    X = Activation(\"relu\")(X)\n",
    "    # Dropout (using rate 0.8)\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    \n",
    "    # Step 2: First GRU Layer\n",
    "    # GRU (use 128 units to return the sequences)\n",
    "    X = GRU(units=128, return_sequences=True)(X)\n",
    "    # Dropout (using rate 0.8)\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    # Batch Normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    # Step 3: Second GRU Layer\n",
    "    # GRU (use 128 units to return the sequences)\n",
    "    X = GRU(units=128, return_sequences=True)(X)\n",
    "    # Dropout (using rate 0.8)\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    # Batch Normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    # Dropout (using rate 0.8)\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    \n",
    "    # Step 4: Time-distributed dense layer\n",
    "    X = TimeDistributed(Dense(1, activation=\"sigmoid\"))(X)\n",
    "    \n",
    "    # Return model\n",
    "    model = Model(inputs = [X_input], outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEfBQCPhR2QQ"
   },
   "source": [
    "Sanity check for `Tx`, `n_freq`, `Ty`. \n",
    "\n",
    "1. Input into model `Tx` and `n_freq`\n",
    "2. Call `model.summary()`\n",
    "3. `Tx` of **sample** and variable = `input_7.shape[1]` (ie column 2, row 1, second element of array)\n",
    "4. `Ty` = `input_7.shape[1]` (ie column 2, row 2, second element of array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqW78CpL8sYH"
   },
   "outputs": [],
   "source": [
    "Tx = 5490 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 129 # Number of frequencies input to the model at each time step of the spectrogram\n",
    "Ty = 1369 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2szTzk7Eh01"
   },
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4o7XUQnaDQp8"
   },
   "outputs": [],
   "source": [
    "model = load_model(\"filler_detection/trained_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEezPsKZEnUh"
   },
   "source": [
    "### Create New Model (if no pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3Sfny3t78rK"
   },
   "outputs": [],
   "source": [
    "model = model(input_shape = (Tx, n_freq))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owDLV1UyENRt"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYzGhjq78vL-"
   },
   "outputs": [],
   "source": [
    "# Function to fit and further train the model\n",
    "def model_train(model, X, Y):\n",
    "    \"\"\"\n",
    "    Function to train the model further using Adam optimiser and binary \n",
    "    cross entropy loss.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- Model to train\n",
    "    X -- X data to train on\n",
    "    Y -- Y data to train on\n",
    "    \"\"\"\n",
    "    \n",
    "    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X, Y, batch_size=5, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Z7c9Pha8wkw"
   },
   "outputs": [],
   "source": [
    "# Train the model on stubbed data downloaded from Coursera\n",
    "# model = load_model(STUB_MODEL)\n",
    "\n",
    "X = np.load(\"/content/drive/My Drive/filler_detection/train_data/XY_Train/X_1.npy\")\n",
    "Y = np.load(\"/content/drive/My Drive/filler_detection/train_data/XY_Train/Y_1.npy\")\n",
    "\n",
    "model_train(model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36S02j3ZEHaB"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9n0vUBlGogT"
   },
   "outputs": [],
   "source": [
    "print(f\"{DATA_PATH}/Y_2.npy\")\n",
    "print(f\"{PROJECT_PATH}/Y_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6W2RaaAM85_Q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to test the model on new data\n",
    "def model_test(model, X_dev, Y_dev):\n",
    "    loss, acc = model.evaluate(X_dev, Y_dev)\n",
    "    print(\"Dev set accuracy = \", acc)\n",
    "\n",
    "X_dev = np.load(f\"{PROJECT_PATH}/X_2.npy\")\n",
    "Y_dev = np.load(f\"{PROJECT_PATH}/Y_2.npy\")\n",
    "print(\"X: {} Y: {}\".format(X_dev.shape, Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python37564bitf304d4ac23234a529ebef17df9391c14"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "my6I-6tlRSuK"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bAbmFvBVRXtY"
      },
      "source": [
        "Syncing from google drive and github... for more info on this code, refer [here](https://zerowithdot.com/colab-github-workflow/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FO4R_OEE3LSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "1a800d9a-69c8-4f9a-9b00-020ccd41b89b"
      },
      "source": [
        "from google.colab import drive\n",
        "from os.path import join\n",
        "\n",
        "ROOT = '/content/drive'     # default for the drive\n",
        "PROJ = 'My Drive/filler_detection/train_data/XY_Train'       # path to your project on Drive\n",
        "PROJ2= 'My Drive/filler_detection/train_data/dev_npy'\n",
        "\n",
        "drive.mount(ROOT)           # we mount the drive at /content/drive\n",
        "\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "PROJCT_PATH2 = join(ROOT, PROJ2)\n",
        "!mkdir \"{PROJECT_PATH}\"     # in case we haven't created it already   \n",
        "!mkdir \"{PROJECT_PATH2}\"\n",
        "\n",
        "GIT_PATH = \"https://github.com/ybchen97/filler_detection.git\"\n",
        "!mkdir ./temp\n",
        "!git clone \"{GIT_PATH}\"\n",
        "!mv ./temp/* \"{PROJECT_PATH}\"\n",
        "!rm -rf ./temp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/filler_detection/train_data/XY_Train’: File exists\n",
            "Cloning into 'filler_detection'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 3849 (delta 11), reused 41 (delta 5), pack-reused 3799\u001b[K\n",
            "Receiving objects: 100% (3849/3849), 176.00 MiB | 25.03 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n",
            "Checking out files: 100% (3600/3600), done.\n",
            "mv: cannot stat './temp/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qa4hVHm5RouU"
      },
      "source": [
        "Install packages in this local notebook specified in requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3SSCyXL8lBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cca50b27-20f4-45d9-9b16-129cde085c76"
      },
      "source": [
        "!pip install -r '/content/filler_detection/requirements.txt'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 1)) (0.9.0)\n",
            "Collecting appnope==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/a9/7985e6a53402f294c8f0e8eff3151a83f1fb901fa92909bb3ff29b4d22af/appnope-0.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 4)) (0.1.0)\n",
            "Collecting cachetools==4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi==2019.11.28 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 6)) (2019.11.28)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 9)) (4.4.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting google-auth==1.11.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.27.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 14)) (1.27.2)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 15)) (2.10.0)\n",
            "Collecting idna==2.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting ipython==7.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/6f/69f1eec859ce48a86660529b166b6ea466f0f4ab98e4fc0807b835aa22c6/ipython-7.13.0-py3-none-any.whl (780kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 18)) (0.2.0)\n",
            "Requirement already satisfied: jedi==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 19)) (0.16.0)\n",
            "Collecting Keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 21)) (1.0.8)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 22)) (1.1.0)\n",
            "Requirement already satisfied: kiwisolver==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 23)) (1.1.0)\n",
            "Requirement already satisfied: Markdown==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 24)) (3.2.1)\n",
            "Collecting matplotlib==3.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/fc/5889757c4c70c552f56fddc8fbdcab565475686cdebdfa1806a9d54cd53b/matplotlib-3.2.0-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4MB 35.4MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 122kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 27)) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum==3.2.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 28)) (3.2.0)\n",
            "Requirement already satisfied: parso==0.6.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 29)) (0.6.2)\n",
            "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 30)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 31)) (0.7.5)\n",
            "Collecting prompt-toolkit==3.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/29/d744cee89937b7e52a5c20ca237a6c77298f757965eb3eb0c653df1bfb14/prompt_toolkit-3.0.4-py3-none-any.whl (351kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 56.9MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 34)) (0.6.0)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 35)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 36)) (0.2.8)\n",
            "Collecting pydub==0.23.1\n",
            "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
            "Collecting Pygments==2.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
            "\u001b[K     |████████████████████████████████| 921kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing==2.4.6 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 39)) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 40)) (2.8.1)\n",
            "Collecting PyYAML==5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 70.4MB/s \n",
            "\u001b[?25hCollecting requests==2.23.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 43)) (1.3.0)\n",
            "Requirement already satisfied: rsa==4.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 44)) (4.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 45)) (1.4.1)\n",
            "Collecting six==1.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorboard==2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 47)) (2.1.1)\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 43kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 50)) (1.1.0)\n",
            "Collecting tf-utils==1.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/bf/f6800f5d34c2563849a900abaa8e8fdb44e859f9a45e45c35afcfd77d742/tf_utils-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 52)) (4.3.3)\n",
            "Collecting urllib3==1.25.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.2MB/s \n",
            "\u001b[?25hCollecting wcwidth==0.1.8\n",
            "  Downloading https://files.pythonhosted.org/packages/58/b4/4850a0ccc6f567cc0ebe7060d20ffd4258b8210efadc259da62dc6ed9c65/wcwidth-0.1.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Werkzeug==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 55)) (1.0.0)\n",
            "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 56)) (1.12.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.11.3->-r /content/filler_detection/requirements.txt (line 11)) (46.0.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.1->-r /content/filler_detection/requirements.txt (line 47)) (0.34.2)\n",
            "Building wheels for collected packages: gast, PyYAML\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=13fc9ba8a34173f3d0208c763e0c5dc662635906b07c9eb5b1cae50117c685bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=beec5278e10c2bf6c7cd582eeaf3695677c53da1f03cc8c92ab6e5a406c721e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "Successfully built gast PyYAML\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.7.2, but you'll have google-auth 1.11.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: appnope, cachetools, gast, six, google-auth, idna, wcwidth, prompt-toolkit, Pygments, ipython, numpy, PyYAML, Keras, matplotlib, protobuf, pydub, urllib3, requests, tensorflow-estimator, tensorflow, tf-utils\n",
            "  Found existing installation: cachetools 3.1.1\n",
            "    Uninstalling cachetools-3.1.1:\n",
            "      Successfully uninstalled cachetools-3.1.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: google-auth 1.7.2\n",
            "    Uninstalling google-auth-1.7.2:\n",
            "      Successfully uninstalled google-auth-1.7.2\n",
            "  Found existing installation: idna 2.8\n",
            "    Uninstalling idna-2.8:\n",
            "      Successfully uninstalled idna-2.8\n",
            "  Found existing installation: wcwidth 0.1.9\n",
            "    Uninstalling wcwidth-0.1.9:\n",
            "      Successfully uninstalled wcwidth-0.1.9\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: numpy 1.18.2\n",
            "    Uninstalling numpy-1.18.2:\n",
            "      Successfully uninstalled numpy-1.18.2\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "  Found existing installation: matplotlib 3.2.1\n",
            "    Uninstalling matplotlib-3.2.1:\n",
            "      Successfully uninstalled matplotlib-3.2.1\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc1\n",
            "    Uninstalling tensorflow-2.2.0rc1:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc1\n",
            "Successfully installed Keras-2.3.1 PyYAML-5.3 Pygments-2.6.1 appnope-0.1.0 cachetools-4.0.0 gast-0.2.2 google-auth-1.11.3 idna-2.9 ipython-7.13.0 matplotlib-3.2.0 numpy-1.18.1 prompt-toolkit-3.0.4 protobuf-3.11.3 pydub-0.23.1 requests-2.23.0 six-1.14.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0 tf-utils-1.0.4 urllib3-1.25.8 wcwidth-0.1.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "cachetools",
                  "google",
                  "idna",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "prompt_toolkit",
                  "pygments",
                  "requests",
                  "six",
                  "urllib3",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A7Hn1LtPRxZS"
      },
      "source": [
        "Importing and setting up env variables..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rasqjnO6mAq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89ba6bb9-d3ed-4761-c473-f38c9d7a75ba"
      },
      "source": [
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import IPython\n",
        "import wave\n",
        "import pylab\n",
        "from tf_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "# Import files for trigger-word detection model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
        "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shTwr2KwE6MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from os.path import join\n",
        "\n",
        "ROOT = '/content/drive'     # default for the drive\n",
        "PROJ = 'My Drive/filler_detection/train_data/XY_Train'       # path to your project on Drive\n",
        "\n",
        "DATA_PATH = join(ROOT, PROJ)\n",
        "REPO = \"/content/filler_detection/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8lgJ6UdH9ViH",
        "colab": {}
      },
      "source": [
        "POSITIVE_DIRECTORY = \"./raw_data/positive_data/\"\n",
        "BACKGROUND_DIRECTORY = \"./raw_data/background_data/\"\n",
        "NEGATIVES_DIRECTORY = \"./raw_data/google_dataset/\"\n",
        "NEGATIVES_TRUNCATED_DIRECTORY = \"./raw_data/google_dataset_truncated/\"\n",
        "AUDIO_EXAMPLES_DIRECTORY = \"./audio_examples/\"\n",
        "POSITIVE_EXAMPLE = \"jh_1.wav\"\n",
        "AUDIO_EXAMPLE = \"example_train.wav\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9eM15MVGR0eJ"
      },
      "source": [
        "The fun begins...\n",
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b54E5A1c8pSh",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(input_shape):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras library.\n",
        "    \n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "    \n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    \n",
        "    # Step 1: CONV Layer\n",
        "    # CONV-1D\n",
        "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # RelU activation\n",
        "    X = Activation(\"relu\")(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=0.8)(X)\n",
        "    \n",
        "    # Step 2: First GRU Layer\n",
        "    # GRU (use 128 units to return the sequences)\n",
        "    X = GRU(units=128, return_sequences=True)(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=0.8)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    \n",
        "    # Step 3: Second GRU Layer\n",
        "    # GRU (use 128 units to return the sequences)\n",
        "    X = GRU(units=128, return_sequences=True)(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=0.8)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=0.8)(X)\n",
        "    \n",
        "    # Step 4: Time-distributed dense layer\n",
        "    X = TimeDistributed(Dense(1, activation=\"sigmoid\"))(X)\n",
        "    \n",
        "    # Return model\n",
        "    model = Model(inputs = [X_input], outputs = X)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oEfBQCPhR2QQ"
      },
      "source": [
        "Sanity check for `Tx`, `n_freq`, `Ty`. \n",
        "\n",
        "1. Input into model `Tx` and `n_freq`\n",
        "2. Call `model.summary()`\n",
        "3. `Tx` of **sample** and variable = `input_7.shape[1]` (ie column 2, row 1, second element of array)\n",
        "4. `Ty` = `input_7.shape[1]` (ie column 2, row 2, second element of array)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqW78CpL8sYH",
        "colab": {}
      },
      "source": [
        "Tx = 5490 # The number of time steps input to the model from the spectrogram\n",
        "n_freq = 129 # Number of frequencies input to the model at each time step of the spectrogram\n",
        "Ty = 1369 # The number of time steps in the output of our model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2szTzk7Eh01",
        "colab_type": "text"
      },
      "source": [
        "### Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o7XUQnaDQp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "7f1760c0-1ad3-45b9-aba4-0fad6a509fb5"
      },
      "source": [
        "model = load_model(\"/content/drive/My Drive/filler_detection/new_trained_model.h5\")\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 5490, 129)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1369, 196)         379456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1369, 196)         784       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1369, 196)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1369, 196)         0         \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 1369, 128)         124800    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1369, 128)         512       \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 1369, 128)         98688     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1369, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 1369, 1)           129       \n",
            "=================================================================\n",
            "Total params: 604,881\n",
            "Trainable params: 603,977\n",
            "Non-trainable params: 904\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEezPsKZEnUh",
        "colab_type": "text"
      },
      "source": [
        "### Create New Model (if not pre-trained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D3Sfny3t78rK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "999abb89-5cf3-493f-a671-78db69447187"
      },
      "source": [
        "model = model(input_shape = (Tx, n_freq))\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 5490, 129)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1369, 196)         379456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1369, 196)         784       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1369, 196)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1369, 196)         0         \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 1369, 128)         124800    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1369, 128)         512       \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 1369, 128)         98688     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1369, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 1369, 1)           129       \n",
            "=================================================================\n",
            "Total params: 604,881\n",
            "Trainable params: 603,977\n",
            "Non-trainable params: 904\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owDLV1UyENRt",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYzGhjq78vL-",
        "colab": {}
      },
      "source": [
        "# Function to fit and further train the model\n",
        "def model_train(model, X, Y):\n",
        "    \"\"\"\n",
        "    Function to train the model further using Adam optimiser and binary \n",
        "    cross entropy loss.\n",
        "    \n",
        "    Arguments:\n",
        "    model -- Model to train\n",
        "    X -- X data to train on\n",
        "    Y -- Y data to train on\n",
        "    \"\"\"\n",
        "    \n",
        "    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(X, Y, batch_size=5, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Z7c9Pha8wkw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05ddd0ed-d07b-463d-8563-97942a148f8e"
      },
      "source": [
        "# Train the model on stubbed data downloaded from Coursera\n",
        "# model = load_model(STUB_MODEL)\n",
        "\n",
        "X = np.load(\"/content/drive/My Drive/filler_detection/train_data/XY_Train/X_2.npy\")\n",
        "Y = np.load(\"/content/drive/My Drive/filler_detection/train_data/XY_Train/Y_2.npy\")\n",
        "\n",
        "model_train(model, X, Y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 961s 961ms/step - loss: 0.9065 - accuracy: 0.7063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZGWdQwefVd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"/content/drive/My Drive/filler_detection/new_trained_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36S02j3ZEHaB",
        "colab_type": "text"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9n0vUBlGogT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "34c47f36-e023-4a57-900f-67546b1658c0"
      },
      "source": [
        "print(f\"{DATA_PATH}/Y_3.npy\")\n",
        "print(f\"{PROJECT_PATH}/Y_3.npy\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/filler_detection/train_data/XY_Train/Y_3.npy\n",
            "/content/drive/My Drive/filler_detection/train_data/XY_Train/Y_3.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6W2RaaAM85_Q",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5e47acf-d113-452b-86de-4e6a6432e7e7"
      },
      "source": [
        "# Function to test the model on new data\n",
        "def model_test(model, X_dev, Y_dev):\n",
        "    loss, acc = model.evaluate(X_dev, Y_dev)\n",
        "    print(\"Dev set accuracy = \", acc)\n",
        "\n",
        "X_dev = np.load(f\"{PROJECT_PATH}/X_3.npy\")\n",
        "Y_dev = np.load(f\"{PROJECT_PATH}/Y_3.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_dev.shape, Y_dev.shape))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: (1000, 5490, 129) Y: (1000, 1369, 1)\n"
          ],
          "name": "stdout"
        }
      ]
>>>>>>> Stashed changes
    },
    "colab_type": "code",
    "id": "F6uTgE6187gq",
    "outputId": "096eb4b1-3745-4919-e529-4c7f87212234"
   },
   "outputs": [
    {
<<<<<<< Updated upstream
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 49s 49ms/step\n",
      "Dev set accuracy =  0.9360423684120178\n"
     ]
=======
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F6uTgE6187gq",
        "outputId": "d43109ef-2ac4-4815-b9ac-2251ee27f5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_test(model, X_dev, Y_dev)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 49s 49ms/step\n",
            "Dev set accuracy =  0.9629291296005249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSKGzFmXsbGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_11 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_11X.npy\")\n",
        "Y_11 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_11Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_11.shape, Y_11.shape))\n",
        "\n",
        "X_12 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_12X.npy\")\n",
        "Y_12 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_12Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_12.shape, Y_12.shape))\n",
        "\n",
        "X_13 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_13X.npy\")\n",
        "Y_13 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_13Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_13.shape, Y_13.shape))\n",
        "\n",
        "X_14 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_14X.npy\")\n",
        "Y_14 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_14Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_14.shape, Y_14.shape))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1qpoTWEIruAz",
        "colab": {}
      },
      "source": [
        "model_test(model, X_11, Y_11)\n",
        "model_test(model, X_12, Y_12)\n",
        "model_test(model, X_13, Y_13)\n",
        "model_test(model, X_14, Y_14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo6H0xFiu-Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_dev_ids = [11,12,13,14,20,21,22,23,24,30,31,32,33,34,35,36,37,38,39,310,40,41,42,43,44]\n",
        "for id in list_of_dev_ids:\n",
        "    X_id = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_\" + str(id) + \"X.npy\")\n",
        "\n",
        "    Y_id = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_\" + str(id) + \"Y.npy\")\n",
        "    print((\"X\" + str(id) + \": {} Y\" + str(id) + \": {}\").format(X_id.shape, Y_id.shape))\n",
        "    model_test(model, X_id, Y_id)\n",
        "\n",
        "model_test(model, X_11, Y_11)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G828z31mLeyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess the audio to the correct format\n",
        "def preprocess_audio(filename):\n",
        "    print(\"PREPROCESSING...\")\n",
        "    # Trim or pad audio segment to 10000ms\n",
        "    padding = AudioSegment.silent(duration=10000)\n",
        "    segment = AudioSegment.from_wav(filename)[:10000]\n",
        "    segment = padding.overlay(segment)\n",
        "    # Set frame rate to 123000\n",
        "    segment = segment.set_channels(1)\n",
        "    segment = segment.set_frame_rate(123000)\n",
        "    # Export as wav\n",
        "    segment.export(filename, format='wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5a7eolILjXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_triggerword(filename):\n",
        "    \"\"\"\n",
        "    Function to take filename and generate a prediction vector.\n",
        "    \n",
        "    Argument:\n",
        "    filename -- Audio file to run prediction on\n",
        "    \n",
        "    Returns:\n",
        "    predictions -- Prediction vector with probabilities\n",
        "    \"\"\"\n",
        "    \n",
        "    sample_rate, samples = wavfile.read(filename)\n",
        "    _, _, x = signal.spectrogram(samples, sample_rate)\n",
        "    print(x.shape)\n",
        "    \n",
        "    # the spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
        "    x  = x.swapaxes(0,1)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    \n",
        "    predictions = model.predict(x)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPGSTQg3LlTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_filler_word(filename, threshold):\n",
        "    \"\"\"\n",
        "    Function to count the number of times trigger word spoken in audio.\n",
        "    \n",
        "    Arguments:\n",
        "    filename -- Audio file to run prediction on\n",
        "    threshold -- Probability above which trigger word considered present\n",
        "    \"\"\"\n",
        "\n",
        "    preprocess_audio(filename)\n",
        "    audio_clip = AudioSegment.from_wav(filename)\n",
        "    chime = AudioSegment.from_wav(CHIME_FILE)\n",
        "    predictions = detect_triggerword(filename)\n",
        "    Ty = predictions.shape[1]\n",
        "    \n",
        "    # Step 1: Initialize the number of consecutive output steps to 0\n",
        "    consecutive_timesteps = 0\n",
        "    # Step 2: Loop over the output steps in the y\n",
        "    for i in range(Ty):\n",
        "        # Step 3: Increment consecutive output steps\n",
        "        consecutive_timesteps += 1\n",
        "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
        "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
        "            # Step 5: Superpose audio and background using pydub\n",
        "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
        "            # Step 6: Reset consecutive output steps to 0\n",
        "            consecutive_timesteps = 0\n",
        "        \n",
        "    audio_clip.export(\"chime_output.wav\", format='wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1pHL08ULqdB",
        "colab_type": "code",
        "outputId": "d5b5930a-5f89-4372-d11c-5db91169f821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "count_filler_word('./Test_2.wav', 0.4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREPROCESSING...\n",
            "(129, 5490)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8ArRym8MOXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
>>>>>>> Stashed changes
    }
   ],
   "source": [
    "model_test(model, X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSKGzFmXsbGA"
   },
   "outputs": [],
   "source": [
    "X_11 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_11X.npy\")\n",
    "Y_11 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_11Y.npy\")\n",
    "print(\"X: {} Y: {}\".format(X_11.shape, Y_11.shape))\n",
    "\n",
    "X_12 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_12X.npy\")\n",
    "Y_12 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_12Y.npy\")\n",
    "print(\"X: {} Y: {}\".format(X_12.shape, Y_12.shape))\n",
    "\n",
    "X_13 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_13X.npy\")\n",
    "Y_13 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_13Y.npy\")\n",
    "print(\"X: {} Y: {}\".format(X_13.shape, Y_13.shape))\n",
    "\n",
    "X_14 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_14X.npy\")\n",
    "Y_14 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_14Y.npy\")\n",
    "print(\"X: {} Y: {}\".format(X_14.shape, Y_14.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qpoTWEIruAz"
   },
   "outputs": [],
   "source": [
    "model_test(model, X_11, Y_11)\n",
    "model_test(model, X_12, Y_12)\n",
    "model_test(model, X_13, Y_13)\n",
    "model_test(model, X_14, Y_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo6H0xFiu-Bq"
   },
   "outputs": [],
   "source": [
    "list_of_dev_ids = [11,12,13,14,20,21,22,23,24,30,31,32,33,34,35,36,37,38,39,310,40,41,42,43,44]\n",
    "for id in list_of_dev_ids:\n",
    "    X_id = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_\" + str(id) + \"X.npy\")\n",
    "\n",
    "    Y_id = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_\" + str(id) + \"Y.npy\")\n",
    "    print((\"X\" + str(id) + \": {} Y\" + str(id) + \": {}\").format(X_id.shape, Y_id.shape))\n",
    "    model_test(model, X_id, Y_id)\n",
    "\n",
    "model_test(model, X_11, Y_11)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
