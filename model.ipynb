{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ybchen97/filler_detection/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "my6I-6tlRSuK"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bAbmFvBVRXtY"
      },
      "source": [
        "Syncing from google drive and github... for more info on this code, refer [here](https://zerowithdot.com/colab-github-workflow/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_uF7CGdfZzs",
        "outputId": "3241d680-be88-4ce7-f996-b20155e28061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from os.path import join\n",
        "\n",
        "ROOT = '/content/drive'     # default for the drive\n",
        "\n",
        "drive.mount(ROOT)           # we mount the drive at /content/drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FO4R_OEE3LSV",
        "outputId": "d3b1413f-a60f-4ada-b60e-110dc2da9293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "GIT_PATH = \"https://github.com/ybchen97/filler_detection.git\"\n",
        "!mkdir ./temp\n",
        "!git clone \"{GIT_PATH}\"\n",
        "!mv ./temp/* \"{PROJECT_PATH}\"\n",
        "!rm -rf ./temp"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'filler_detection'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 4976 (delta 52), reused 35 (delta 19), pack-reused 4887\u001b[K\n",
            "Receiving objects: 100% (4976/4976), 841.51 MiB | 34.70 MiB/s, done.\n",
            "Resolving deltas: 100% (498/498), done.\n",
            "mv: cannot stat './temp/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qa4hVHm5RouU"
      },
      "source": [
        "Install packages in this local notebook specified in requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3SSCyXL8lBb",
        "outputId": "37662a54-283e-4f15-813c-7bc1a8c53177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r '/content/filler_detection/requirements.txt'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 1)) (0.9.0)\n",
            "Collecting appdirs==1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting appnope==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/a9/7985e6a53402f294c8f0e8eff3151a83f1fb901fa92909bb3ff29b4d22af/appnope-0.1.0-py2.py3-none-any.whl\n",
            "Collecting astetik==1.9.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/ba/f8622951da73d9b47b45bb847112c388651f9c6e413e712954f260301d9f/astetik-1.9.9.tar.gz\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 5)) (0.8.1)\n",
            "Collecting astroid==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 16.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs==19.3.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 7)) (19.3.0)\n",
            "Collecting autopep8==1.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/f3/24b437da561b6af4840c871fbbda32889ca304fc1f7b6cc3ada8b09f394a/autopep8-1.4.4.tar.gz (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 9)) (0.1.0)\n",
            "Requirement already satisfied: bleach==3.1.4 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 10)) (3.1.4)\n",
            "Collecting cachetools==4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
            "Collecting certifi==2019.11.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 19.2MB/s \n",
            "\u001b[?25hCollecting chances==0.1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 14)) (3.0.4)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 15)) (0.10.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 16)) (4.4.2)\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 17)) (0.6.0)\n",
            "Collecting distlib==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/29/694a3a4d7c0e1aef76092e9167fbe372e0f7da055f5dcf4e1313ec21d96a/distlib-0.3.0.zip (571kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 19)) (0.3)\n",
            "Collecting essential-generators==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/b1/979b823497488e5f13c9070fcd6a2e24f6d9c6fd5398e0fbeccc8158bd3b/essential_generators-0.9.2-py3-none-any.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 19.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 21)) (3.0.12)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting geonamescache==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c1/efb823270c8526b2f4f3eb8c804c5a0a55277267ad2312f5eb47bd9cc370/geonamescache-1.1.0-py3-none-any.whl (830kB)\n",
            "\u001b[K     |████████████████████████████████| 839kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core==1.16.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 24)) (1.16.0)\n",
            "Collecting google-auth==1.11.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 26)) (0.4.1)\n",
            "Collecting google-cloud-texttospeech==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/31/775c5ef2e4be346f822f80818cae6000ea7bd352685e5c32736e6c190983/google_cloud_texttospeech-1.0.1-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 28)) (0.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.51.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 29)) (1.51.0)\n",
            "Collecting grpcio==1.27.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/df/1f8a284a5e5819ae07d50bd76996d6f7208afef7533e4896fa1c6445574f/grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 31)) (2.10.0)\n",
            "Collecting idna==2.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/03/a00d504808808912751e64ccf414be53c29cad620e3de2421135fcae3025/importlib_metadata-1.5.0-py2.py3-none-any.whl\n",
            "Collecting ipykernel==5.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/62/d1a5d654b7a21bd3eb99be1b59a608cc18a7a08ed88495457a87c40a0495/ipykernel-5.1.4-py3-none-any.whl (116kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 58.4MB/s \n",
            "\u001b[?25hCollecting ipython==7.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/6f/69f1eec859ce48a86660529b166b6ea466f0f4ab98e4fc0807b835aa22c6/ipython-7.13.0-py3-none-any.whl (780kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 36)) (0.2.0)\n",
            "Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 37)) (7.5.1)\n",
            "Collecting isort==4.3.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hCollecting jedi==0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/67/333e2196b70840f411fd819407b4e98aa3150c2bd24c52154a451f912ef2/jedi-0.16.0-py2.py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.4MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/24/4f35961e5c669e96f6559760042a55b9bcfcdb82b9bdb3c8753dbe042e35/Jinja2-2.11.1-py2.py3-none-any.whl (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib==0.14.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 41)) (0.14.1)\n",
            "Collecting jsonschema==3.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hCollecting jupyter-client==6.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/75/4c4eb43749e59db3c1c7932b50eaf8c4b8219b1b5644fe379ea796f8dbe5/jupyter_client-6.0.0-py3-none-any.whl (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 57.0MB/s \n",
            "\u001b[?25hCollecting jupyter-console==6.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/89/742fa5a80b552ffcb6a8922712697c6e6828aee7b91ee4ae2b79f00f8401/jupyter_console-6.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-core==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 45)) (4.6.3)\n",
            "Requirement already satisfied: Keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 46)) (2.3.1)\n",
            "Requirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 47)) (1.0.8)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 48)) (1.1.0)\n",
            "Collecting kerasplotlib==0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b7/31663d3b5ea9afd8c2c6ffa06d3c4e118ef363e12dc75b7c49fb6a2d22aa/kerasplotlib-0.1.6.tar.gz\n",
            "Collecting kiwisolver==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.1MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/2a/d73b99e9407be3acd7c0328fcc44bcf6f5c42e6d03d1fb192032c0057d13/lazy_object_proxy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 52)) (3.2.1)\n",
            "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 53)) (1.1.1)\n",
            "Collecting matplotlib==3.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/fc/5889757c4c70c552f56fddc8fbdcab565475686cdebdfa1806a9d54cd53b/matplotlib-3.2.0-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4MB 129kB/s \n",
            "\u001b[?25hCollecting mccabe==0.6.1\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 56)) (0.8.4)\n",
            "Collecting Naked==0.1.31\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/36/b8107b51adca73402ec1860d88f41d958e275e60eea6eeaa9c39ddb89a40/Naked-0.1.31-py2.py3-none-any.whl (590kB)\n",
            "\u001b[K     |████████████████████████████████| 593kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbconvert==5.6.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 58)) (5.6.1)\n",
            "Collecting nbformat==5.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/eb/de575b7a64e7ab8d8c95e4c180ccc36deda3f1379186c4ee7adf6c2f1586/nbformat-5.0.4-py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 45.5MB/s \n",
            "\u001b[?25hCollecting notebook==6.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/f1/0a67f09ef53a342403ffa66646ee39273e0ac79ffa5de5dbe2f3e28b5bdf/notebook-6.0.3-py3-none-any.whl (9.7MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7MB 43.4MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 160kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 62)) (3.1.0)\n",
            "Collecting opt-einsum==3.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/49/2233e63052d5686c72131b579837ddfb98ba9dd0b92bb91efcb441ada8ce/opt_einsum-3.2.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas==1.0.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 64)) (1.0.3)\n",
            "Requirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 65)) (1.4.2)\n",
            "Collecting parso==0.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/15/888f80e429a971d3838124adde705d7b07650aa3a59f4db07d61f653b8cd/parso-0.6.2-py2.py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 67)) (0.5.1)\n",
            "Collecting pep8==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 69)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 70)) (0.7.5)\n",
            "Requirement already satisfied: prometheus-client==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 71)) (0.7.1)\n",
            "Collecting prompt-toolkit==3.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/29/d744cee89937b7e52a5c20ca237a6c77298f757965eb3eb0c653df1bfb14/prompt_toolkit-3.0.4-py3-none-any.whl (351kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 42.6MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 74)) (0.6.0)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 75)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 76)) (0.2.8)\n",
            "Collecting pycodestyle==2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/0c/04a353e104d2f324f8ee5f4b32012618c1c86dd79e52a433b64fceed511b/pycodestyle-2.5.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hCollecting pycryptodome==3.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/43/1598eb29a97cfbeb370f09936c64add9109a764acf30fd3ff876b5d3f8ed/pycryptodome-3.9.1-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7MB 39.3MB/s \n",
            "\u001b[?25hCollecting pydub==0.23.1\n",
            "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
            "Collecting Pygments==2.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
            "\u001b[K     |████████████████████████████████| 921kB 46.9MB/s \n",
            "\u001b[?25hCollecting pylint==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl (765kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 26.9MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/bc/1e58593167fade7b544bfe9502a26dc860940a79ab306e651e7f13be68c2/pyparsing-2.4.6-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting pyrsistent==0.15.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/aa/cdcf7ef88cc0f831b6f14c8c57318824c9de9913fe8de38e46a98c069a35/pyrsistent-0.15.7.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 84)) (2.8.1)\n",
            "Collecting pytz==2019.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 24.2MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq==19.0.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 87)) (19.0.0)\n",
            "Collecting qtconsole==4.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/de/2a0bda85367881e27370a206a561326a99fbb05ab9402f4c4ad59761eec4/qtconsole-4.7.1-py2.py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 17.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: QtPy==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 89)) (1.9.0)\n",
            "Collecting requests==2.23.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 91)) (1.3.0)\n",
            "Requirement already satisfied: rsa==4.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 92)) (4.0)\n",
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 93)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 94)) (1.4.1)\n",
            "Requirement already satisfied: seaborn==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 95)) (0.10.0)\n",
            "Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 96)) (1.5.0)\n",
            "Collecting shellescape==3.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/51/b6/986c99a10040beaaefca1ad6c93bd7738cb8e4f52f6caed13d3ed1caa7e4/shellescape-3.4.1-py2.py3-none-any.whl\n",
            "Collecting six==1.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 99)) (0.0)\n",
            "Collecting statsmodels==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 371kB/s \n",
            "\u001b[?25hCollecting talos==0.6.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/64/65bee9b585f6a196ff3119a60a7170fe0b2a18225a2e1e67115510b46a65/talos-0.6.6-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 10.3MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 41kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 105)) (1.1.0)\n",
            "Requirement already satisfied: terminado==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 106)) (0.8.3)\n",
            "Requirement already satisfied: testpath==0.4.4 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 107)) (0.4.4)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 108)) (1.3)\n",
            "Collecting tf-utils==1.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/bf/f6800f5d34c2563849a900abaa8e8fdb44e859f9a45e45c35afcfd77d742/tf_utils-1.0.4-py3-none-any.whl\n",
            "Collecting tornado==6.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/84/119a46d494f008969bf0c775cb2c6b3579d3c4cc1bb1b41a022aa93ee242/tornado-6.0.4.tar.gz (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 44.5MB/s \n",
            "\u001b[?25hCollecting tqdm==4.45.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 112)) (4.3.3)\n",
            "Collecting typed-ast==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 39.8MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 51.3MB/s \n",
            "\u001b[?25hCollecting virtualenv==20.0.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/34/ae98cb0c3eca73b871d51b8f27af0389c746f0e166cd3b2ab31f41085b82/virtualenv-20.0.17-py2.py3-none-any.whl (4.6MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6MB 33.3MB/s \n",
            "\u001b[?25hCollecting wcwidth==0.1.8\n",
            "  Downloading https://files.pythonhosted.org/packages/58/b4/4850a0ccc6f567cc0ebe7060d20ffd4258b8210efadc259da62dc6ed9c65/wcwidth-0.1.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 117)) (0.5.1)\n",
            "Collecting Werkzeug==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/a5/d6f8a6e71f15364d35678a4ec8a0186f980b3bd2545f40ad51dd26a87fb1/Werkzeug-1.0.0-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 119)) (3.5.1)\n",
            "Collecting wrangle==0.6.7\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 121)) (1.12.1)\n",
            "Requirement already satisfied: zipp==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/filler_detection/requirements.txt (line 122)) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core==1.16.0->-r /content/filler_detection/requirements.txt (line 24)) (46.1.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.1->-r /content/filler_detection/requirements.txt (line 102)) (0.34.2)\n",
            "Collecting importlib-resources<2,>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/36/e678bc02cad98e566fbd7e6711ab2f389fc9adfb1edbb9f553eae41f9f97/importlib_resources-1.4.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: astetik, autopep8, chances, distlib, gast, kerasplotlib, pyrsistent, PyYAML, tornado, wrangle\n",
            "  Building wheel for astetik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astetik: filename=astetik-1.9.9-cp36-none-any.whl size=56960 sha256=1c3f4727ffdf5ebf389da20448afc02356d4cb6eecae7724583f2b2dbb5dcb52\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/70/21/c475cd079ec401dd6e1b9b1d42b4c38554ce12679bfb214aad\n",
            "  Building wheel for autopep8 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autopep8: filename=autopep8-1.4.4-py2.py3-none-any.whl size=42631 sha256=704d69f14f5972ea86b53314e5d6c1ca61751baadccb95734eca270794331df2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/f5/4b/c19e6276126325eb8071b273347c05a830c37a82b9b3b81510\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.9-cp36-none-any.whl size=41609 sha256=3a8f30efc33bc3d1b75a663d654286b62ff3651ba65079d49fc06ba67f446026\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n",
            "  Building wheel for distlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distlib: filename=distlib-0.3.0-cp36-none-any.whl size=340429 sha256=555fdd4cad1722679fb370ad24ea37c50f1b3a79ee6bc840b5d1a1aca4713683\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/e8/db/c73dae4867666e89ba3cfbc4b5c092446f0e584eda6f409cbb\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=4616b77a38e37a4c8a40307cef02d91cb7deff5296c5e4a8c07b4c54d26b3f03\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-cp36-none-any.whl size=3601 sha256=9b51a2ef919aebc464f2976091122580c0cc8ed193e490d9662f58ec908903cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/d3/8c/9503a22b0a38e8b21c70ad834e4606d209193443e5c709305d\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsistent: filename=pyrsistent-0.15.7-cp36-cp36m-linux_x86_64.whl size=97687 sha256=90cfeaa56b929e7a674fe7f80579d1bceaabf562cadf5bffc474e4c0469beb26\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/78/ac/f26a78a989cd97f90981d96a560d7e1da5e1307284301d94e8\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=374b02b05a0ae35241e4a1ba68339e5816994184b73e36956650370b7aefc922\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.4-cp36-cp36m-linux_x86_64.whl size=427631 sha256=3d75527f2586903cd343a36dddc4065fb2d3e68e65259c27e0b7f88063ed4628\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/84/2f/409c7b2bb3afc3aa727f7ee8787975e0793f74d1165f4d0104\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp36-none-any.whl size=49894 sha256=4e496d7457fcbbf8c9302fa181df6426993c5bcd1674e0796c2ac6680cbec163\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "Successfully built astetik autopep8 chances distlib gast kerasplotlib pyrsistent PyYAML tornado wrangle\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.7.2, but you'll have google-auth 1.11.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.1.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.2.0, but you'll have notebook 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: talos 0.6.6 has requirement keras==2.3.0, but you'll have keras 2.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: talos 0.6.6 has requirement tensorflow==1.14.0, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: appdirs, appnope, numpy, statsmodels, wrangle, geonamescache, astetik, typed-ast, six, lazy-object-proxy, astroid, pycodestyle, autopep8, cachetools, certifi, idna, urllib3, requests, chances, distlib, essential-generators, gast, google-auth, google-cloud-texttospeech, grpcio, importlib-metadata, tornado, jupyter-client, Pygments, parso, jedi, wcwidth, prompt-toolkit, ipython, ipykernel, isort, Jinja2, pyrsistent, jsonschema, jupyter-console, kerasplotlib, kiwisolver, pyparsing, matplotlib, mccabe, PyYAML, Naked, nbformat, notebook, opt-einsum, pep8, protobuf, pycryptodome, pydub, pylint, pytz, qtconsole, shellescape, tqdm, tensorflow-estimator, Werkzeug, tensorboard, tensorflow, talos, tf-utils, importlib-resources, virtualenv\n",
            "  Found existing installation: numpy 1.18.3\n",
            "    Uninstalling numpy-1.18.3:\n",
            "      Successfully uninstalled numpy-1.18.3\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: cachetools 3.1.1\n",
            "    Uninstalling cachetools-3.1.1:\n",
            "      Successfully uninstalled cachetools-3.1.1\n",
            "  Found existing installation: certifi 2020.4.5.1\n",
            "    Uninstalling certifi-2020.4.5.1:\n",
            "      Successfully uninstalled certifi-2020.4.5.1\n",
            "  Found existing installation: idna 2.8\n",
            "    Uninstalling idna-2.8:\n",
            "      Successfully uninstalled idna-2.8\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: google-auth 1.7.2\n",
            "    Uninstalling google-auth-1.7.2:\n",
            "      Successfully uninstalled google-auth-1.7.2\n",
            "  Found existing installation: grpcio 1.28.1\n",
            "    Uninstalling grpcio-1.28.1:\n",
            "      Successfully uninstalled grpcio-1.28.1\n",
            "  Found existing installation: importlib-metadata 1.6.0\n",
            "    Uninstalling importlib-metadata-1.6.0:\n",
            "      Successfully uninstalled importlib-metadata-1.6.0\n",
            "  Found existing installation: tornado 4.5.3\n",
            "    Uninstalling tornado-4.5.3:\n",
            "      Successfully uninstalled tornado-4.5.3\n",
            "  Found existing installation: jupyter-client 5.3.4\n",
            "    Uninstalling jupyter-client-5.3.4:\n",
            "      Successfully uninstalled jupyter-client-5.3.4\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: parso 0.7.0\n",
            "    Uninstalling parso-0.7.0:\n",
            "      Successfully uninstalled parso-0.7.0\n",
            "  Found existing installation: jedi 0.17.0\n",
            "    Uninstalling jedi-0.17.0:\n",
            "      Successfully uninstalled jedi-0.17.0\n",
            "  Found existing installation: wcwidth 0.1.9\n",
            "    Uninstalling wcwidth-0.1.9:\n",
            "      Successfully uninstalled wcwidth-0.1.9\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Found existing installation: Jinja2 2.11.2\n",
            "    Uninstalling Jinja2-2.11.2:\n",
            "      Successfully uninstalled Jinja2-2.11.2\n",
            "  Found existing installation: pyrsistent 0.16.0\n",
            "    Uninstalling pyrsistent-0.16.0:\n",
            "      Successfully uninstalled pyrsistent-0.16.0\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: jupyter-console 5.2.0\n",
            "    Uninstalling jupyter-console-5.2.0:\n",
            "      Successfully uninstalled jupyter-console-5.2.0\n",
            "  Found existing installation: kiwisolver 1.2.0\n",
            "    Uninstalling kiwisolver-1.2.0:\n",
            "      Successfully uninstalled kiwisolver-1.2.0\n",
            "  Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Found existing installation: matplotlib 3.2.1\n",
            "    Uninstalling matplotlib-3.2.1:\n",
            "      Successfully uninstalled matplotlib-3.2.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: nbformat 5.0.6\n",
            "    Uninstalling nbformat-5.0.6:\n",
            "      Successfully uninstalled nbformat-5.0.6\n",
            "  Found existing installation: notebook 5.2.2\n",
            "    Uninstalling notebook-5.2.2:\n",
            "      Successfully uninstalled notebook-5.2.2\n",
            "  Found existing installation: opt-einsum 3.2.1\n",
            "    Uninstalling opt-einsum-3.2.1:\n",
            "      Successfully uninstalled opt-einsum-3.2.1\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: qtconsole 4.7.3\n",
            "    Uninstalling qtconsole-4.7.3:\n",
            "      Successfully uninstalled qtconsole-4.7.3\n",
            "  Found existing installation: tqdm 4.38.0\n",
            "    Uninstalling tqdm-4.38.0:\n",
            "      Successfully uninstalled tqdm-4.38.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed Jinja2-2.11.1 Naked-0.1.31 PyYAML-5.3 Pygments-2.6.1 Werkzeug-1.0.0 appdirs-1.4.3 appnope-0.1.0 astetik-1.9.9 astroid-2.2.5 autopep8-1.4.4 cachetools-4.0.0 certifi-2019.11.28 chances-0.1.9 distlib-0.3.0 essential-generators-0.9.2 gast-0.2.2 geonamescache-1.1.0 google-auth-1.11.3 google-cloud-texttospeech-1.0.1 grpcio-1.27.2 idna-2.9 importlib-metadata-1.5.0 importlib-resources-1.4.0 ipykernel-5.1.4 ipython-7.13.0 isort-4.3.21 jedi-0.16.0 jsonschema-3.2.0 jupyter-client-6.0.0 jupyter-console-6.1.0 kerasplotlib-0.1.6 kiwisolver-1.1.0 lazy-object-proxy-1.4.1 matplotlib-3.2.0 mccabe-0.6.1 nbformat-5.0.4 notebook-6.0.3 numpy-1.18.1 opt-einsum-3.2.0 parso-0.6.2 pep8-1.7.1 prompt-toolkit-3.0.4 protobuf-3.11.3 pycodestyle-2.5.0 pycryptodome-3.9.1 pydub-0.23.1 pylint-2.3.1 pyparsing-2.4.6 pyrsistent-0.15.7 pytz-2019.3 qtconsole-4.7.1 requests-2.23.0 shellescape-3.4.1 six-1.14.0 statsmodels-0.11.1 talos-0.6.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 tf-utils-1.0.4 tornado-6.0.4 tqdm-4.45.0 typed-ast-1.4.0 urllib3-1.25.8 virtualenv-20.0.17 wcwidth-0.1.8 wrangle-0.6.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "cachetools",
                  "certifi",
                  "google",
                  "grpc",
                  "idna",
                  "ipykernel",
                  "jupyter_client",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "prompt_toolkit",
                  "pygments",
                  "pyparsing",
                  "pytz",
                  "requests",
                  "six",
                  "tornado",
                  "tqdm",
                  "urllib3",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A7Hn1LtPRxZS"
      },
      "source": [
        "Importing and setting up env variables..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rasqjnO6mAq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "047b5d21-9873-42c4-d9c8-ef71210a0953"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337) # for reproducibility\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import IPython\n",
        "import wave\n",
        "import pylab\n",
        "from tf_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "import pickle\n",
        "\n",
        "# Import files for trigger-word detection model\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
        "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape, ConvLSTM2D, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks.callbacks import EarlyStopping\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tsvlLoY6J2Lf",
        "colab": {}
      },
      "source": [
        "PROJ_PATH = '/content/drive/My Drive/filler_detection/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uEezPsKZEnUh"
      },
      "source": [
        "### Create New Model (if not pre-trained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oEfBQCPhR2QQ"
      },
      "source": [
        "Sanity check for `Tx`, `n_freq`, `Ty`. \n",
        "\n",
        "1. Input into model `Tx` and `n_freq`\n",
        "2. Call `model.summary()`\n",
        "3. `Tx` of **sample** and variable = `input_7.shape[1]` (ie column 2, row 1, second element of array)\n",
        "4. `Ty` = `input_7.shape[1]` (ie column 2, row 2, second element of array)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqW78CpL8sYH",
        "colab": {}
      },
      "source": [
        "Tx = 5490 # The number of time steps input to the model from the spectrogram\n",
        "n_freq = 129 # Number of frequencies input to the model at each time step of the spectrogram\n",
        "Ty = 1369 # The number of time steps in the output of our model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b54E5A1c8pSh",
        "colab": {}
      },
      "source": [
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# GRADED FUNCTION: model\n",
        "\n",
        "def create_gru(input_shape, output_bias=None, dropout=0.8):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras library.\n",
        "    \n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "    \n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    \n",
        "    # Step 1: CONV Layer\n",
        "    # CONV-1D\n",
        "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # RelU activation\n",
        "    X = Activation(\"relu\")(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 2: First GRU Layer\n",
        "    # GRU (use 128 units to return the sequences)\n",
        "    X = GRU(units=128, return_sequences=True)(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    \n",
        "    # Step 3: Second GRU Layer\n",
        "    # GRU (use 128 units to return the sequences)\n",
        "    X = GRU(units=128, return_sequences=True)(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 4: Time-distributed dense layer\n",
        "    if output_bias != None: \n",
        "        bias_initializer = RandomNormal(mean=output_bias) \n",
        "    else: \n",
        "        bias_initializer = \"zeros\"\n",
        "    X = TimeDistributed(Dense(1, activation=\"sigmoid\", bias_initializer= bias_initializer))(X)\n",
        "    \n",
        "    # Return model\n",
        "    model = Model(inputs = [X_input], outputs = X)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQrIOKpzvO_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# GRADED FUNCTION: model\n",
        "\n",
        "def create_lstm(input_shape, output_bias=None, dropout=0.8):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras library.\n",
        "    \n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "    \n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    \n",
        "    # Step 1: CONV Layer\n",
        "    # CONV-1D\n",
        "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # RelU activation\n",
        "    X = Activation(\"relu\")(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 2: First bi-lstm Layer\n",
        "    # bi-lsem (use 128 units to return the sequences)\n",
        "    X = LSTM(units=128, return_sequences=True)(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    \n",
        "    # Step 3: Second bi-lstm Layer\n",
        "    # lstm (use 128 units to return the sequences)\n",
        "    X = LSTM(units=128, return_sequences=True)(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 4: Time-distributed dense layer\n",
        "    if output_bias != None: \n",
        "        bias_initializer = RandomNormal(mean=output_bias) \n",
        "    else: \n",
        "        bias_initializer = \"zeros\"\n",
        "    X = TimeDistributed(Dense(1, activation=\"sigmoid\", bias_initializer= bias_initializer))(X)\n",
        "    \n",
        "    # Return model\n",
        "    model = Model(inputs = [X_input], outputs = X)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lYK1QniiDye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# GRADED FUNCTION: model\n",
        "\n",
        "def create_bilstm(input_shape, output_bias=None, dropout=0.8):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras library.\n",
        "    \n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "    \n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    \n",
        "    # Step 1: CONV Layer\n",
        "    # CONV-1D\n",
        "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # RelU activation\n",
        "    X = Activation(\"relu\")(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 2: First bi-lstm Layer\n",
        "    # bi-lsem (use 128 units to return the sequences)\n",
        "    X = Bidirectional(LSTM(units=128, return_sequences=True))(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    \n",
        "    # Step 3: Second bi-lstm Layer\n",
        "    # lstm (use 128 units to return the sequences)\n",
        "    X = Bidirectional(LSTM(units=128, return_sequences=True))(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 4: Time-distributed dense layer\n",
        "    if output_bias != None: \n",
        "        bias_initializer = RandomNormal(mean=output_bias) \n",
        "    else: \n",
        "        bias_initializer = \"zeros\"\n",
        "    X = TimeDistributed(Dense(1, activation=\"sigmoid\", bias_initializer= bias_initializer))(X)\n",
        "    \n",
        "    # Return model\n",
        "    model = Model(inputs = [X_input], outputs = X)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2kYYSD0Ptgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# GRADED FUNCTION: model\n",
        "\n",
        "def create_convlstm2d(input_shape, output_bias=None, dropout=0.8):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras library.\n",
        "    \n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "    \n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    X = Lambda(lambda x: K.expand_dims(x, axis = 2))(X_input)\n",
        "    X = Lambda(lambda x: K.expand_dims(x, axis = 1))(X)\n",
        "    # X = K.expand_dims(X, axis = 1)\n",
        "    print(X)\n",
        "    \n",
        "    # Step 2: First bi-lstm Layer\n",
        "    # bi-lsem (use 128 units to return the sequences)\n",
        "    X = Bidirectional(ConvLSTM2D(filters=196, kernel_size=(15, 1), strides=4, return_sequences=True))(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    \n",
        "    # Step 3: Second bi-lstm Layer\n",
        "    # lstm (use 128 units to return the sequences)\n",
        "    X = Bidirectional(ConvLSTM2D(filters=196, kernel_size=(1, 1), strides=1, return_sequences=True))(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    # Batch Normalization\n",
        "    X = BatchNormalization()(X)\n",
        "    # Dropout (using rate 0.8)\n",
        "    X = Dropout(rate=dropout)(X)\n",
        "    \n",
        "    # Step 4: Time-distributed dense layer\n",
        "    if output_bias != None: \n",
        "        bias_initializer = RandomNormal(mean=output_bias) \n",
        "    else: \n",
        "        bias_initializer = \"zeros\"\n",
        "    X = TimeDistributed(Dense(1, activation=\"sigmoid\", bias_initializer= bias_initializer))(X)\n",
        "    X = Reshape((Ty, 1))(X)\n",
        "    \n",
        "    # Return model\n",
        "    model = Model(inputs = [X_input], outputs = X)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D3Sfny3t78rK",
        "outputId": "e1a8869b-9ac7-4a26-fb5f-667669bd87a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "model = create_lstm(input_shape = (Tx, n_freq))\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 5490, 129)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1369, 196)         379456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1369, 196)         784       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1369, 196)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1369, 196)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 1369, 128)         166400    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1369, 128)         512       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 1369, 128)         131584    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1369, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1369, 128)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 1369, 1)           129       \n",
            "=================================================================\n",
            "Total params: 679,377\n",
            "Trainable params: 678,473\n",
            "Non-trainable params: 904\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-2szTzk7Eh01"
      },
      "source": [
        "### Load Weights of Pre-trained Model\n",
        "Loaded weights instead of model due to custom metrics causing error in loading model directly. Read more [here](https://github.com/keras-team/keras/issues/10104)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rv8qz1uFd9J1",
        "colab": {}
      },
      "source": [
        "model.load_weights(PROJ_PATH + \"models/bryan_val_syn.h5\") \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "owDLV1UyENRt"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bTjJLteo2sNW",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYzGhjq78vL-",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "# Function to fit and further train the model. Returns history (check documentation of `model.fit` for more details).\n",
        "def model_train(model, X, Y, X_dev, Y_dev):\n",
        "    \"\"\"\n",
        "    Function to train the model further using Adam optimiser and binary \n",
        "    cross entropy loss.\n",
        "    \n",
        "    Arguments:\n",
        "    model -- Model to train\n",
        "    X -- X data to train on\n",
        "    Y -- Y data to train on\n",
        "    \"\"\"\n",
        "\n",
        "    METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "    ]\n",
        "\n",
        "    es = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                            min_delta=0.05, \n",
        "                                            patience=100, \n",
        "                                            verbose=1, \n",
        "                                            mode='auto', \n",
        "                                            baseline=None, \n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "    \n",
        "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "    model.compile(loss=f1_loss, optimizer=opt, metrics=METRICS)\n",
        "    \n",
        "    history = model.fit(X, Y, batch_size=50, epochs=100, validation_data=(X_dev, Y_dev), callbacks = [es])\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uHf7t1TjLvnl",
        "outputId": "b97f4f04-ec59-41cb-feea-df44190aadaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = np.load(PROJ_PATH + \"data/train/bryan_lectures/X.npy\")\n",
        "Y = np.load(PROJ_PATH + \"data/train/bryan_lectures/Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X.shape, Y.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: (1000, 5490, 129) Y: (1000, 1369, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7x8yCWQPSCh",
        "outputId": "b7f30d64-e8f2-43a2-d3c6-054ebf6ac547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_dev = np.load(PROJ_PATH + \"data/dev_synthesised/bryan_lectures/X_dev.npy\")\n",
        "Y_dev = np.load(PROJ_PATH + \"data/dev_synthesised/bryan_lectures/Y_dev.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_dev.shape, Y_dev.shape))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: (100, 5490, 129) Y: (100, 1369, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Z7c9Pha8wkw",
        "outputId": "fc4d13c8-9437-467b-d2c9-4a4e76737bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "model_filename = \"bryan_lstm\"\n",
        "history_filename = \"bryan_lstm\"\n",
        "\n",
        "history = model_train(model, X, Y, X_dev, Y_dev)\n",
        "\n",
        "# Save model\n",
        "model.save(PROJ_PATH + f\"models/{model_filename}.h5\")\n",
        "print(\"model saved!\")\n",
        "\n",
        "# Save history\n",
        "with open(PROJ_PATH + f'models/history/{history_filename}', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "print(\"history saved!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e889ad9e8b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhistory_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bryan_lstm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "36S02j3ZEHaB"
      },
      "source": [
        "## Model Evaluation Tools\n",
        "1. Metrics (accuracy, f1, precision, recall). \n",
        "2. Visualising model's prediction\n",
        "3. Visualising `Y.npy` to know where the ones are inserted.\n",
        "4. Plot of metrics against epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6W2RaaAM85_Q",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "X_test = np.load(PROJ_PATH + \"data/test/bryan_synthesised/X_test.npy\")\n",
        "Y_test = np.load(PROJ_PATH + \"data/test/bryan_synthesised/Y_test.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_test.shape, Y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LxjxLRvdCF0G"
      },
      "source": [
        "### Plot of metrics against epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fA84Xkm5-Ddb",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "\n",
        "    fig, axs = plt.subplots(2,3)\n",
        "    fig.set_size_inches(15,7.5,forward=True)\n",
        "    fig.tight_layout(pad=3.0)\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    axs[0,0].plot(history['accuracy'])\n",
        "    axs[0,0].plot(history['val_accuracy'])\n",
        "    axs[0,0].set_title('Model accuracy')\n",
        "    axs[0,0].set_ylabel('Accuracy')\n",
        "    axs[0,0].set_xlabel('Epoch')\n",
        "    axs[0,0].legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    axs[0,1].plot(history['loss'])\n",
        "    axs[0,1].plot(history['val_loss'])\n",
        "    axs[0,1].set_title('Model loss')\n",
        "    axs[0,1].set_ylabel('Loss')\n",
        "    axs[0,1].set_xlabel('Epoch')\n",
        "    axs[0,1].legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "    # Plot precision loss values\n",
        "    axs[1,0].plot(history['precision'])\n",
        "    axs[1,0].plot(history['val_precision'])\n",
        "    axs[1,0].set_title('Model Precision')\n",
        "    axs[1,0].set_ylabel('Precision')\n",
        "    axs[1,0].set_xlabel('Epoch')\n",
        "    axs[1,0].legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "    # Plot recall loss values\n",
        "    axs[1,1].plot(history['recall'])\n",
        "    axs[1,1].plot(history['val_recall'])\n",
        "    axs[1,1].set_title('Model Recall')\n",
        "    axs[1,1].set_ylabel('Recall')\n",
        "    axs[1,1].set_xlabel('Epoch')\n",
        "    axs[1,1].legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "    axs[0,2].plot(history['auc'])\n",
        "    axs[0,2].plot(history['val_auc'])\n",
        "    axs[0,2].set_title('Model AUC')\n",
        "    axs[0,2].set_ylabel('AUC')\n",
        "    axs[0,2].set_xlabel('Epoch')\n",
        "    axs[0,2].legend(['Train', 'Val'], loc='upper right')\n",
        "\n",
        "    axs[1,2].set_visible(not axs[1,2].get_visible())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UBLS-haQgjZq",
        "colab": {}
      },
      "source": [
        "# Load history CHANGE HISTORY PATH HERE\n",
        "history_filename = \"bryan_val_syn\"\n",
        "history = pickle.load(open(PROJ_PATH +f'models/history/{history_filename}', \"rb\"))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Plm76s9Bewbi"
      },
      "source": [
        "### Outputs accuracy, F1 score, precision, recall scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rKfvIuGq7W7N",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "\n",
        "# Evaluates the model on the test sets based on accuracy, precision, recall, and f1 score.\n",
        "def evaluate_model(model, X_test, Y_test, threshold=0.5):\n",
        "    # predict probabilities for dev set\n",
        "    Y_predict = model.predict(X_test, verbose=0)\n",
        "    # Applying sigmoid onto prediction of probabilities in order to use f-score\n",
        "    Y_predict = np.where(Y_predict > 0.3, 1, 0)\n",
        "\n",
        "    # flatten array in order to use fscore\n",
        "    Y_test_flattened = Y_test.flatten()\n",
        "    Y_predict_flattened = Y_predict.flatten()\n",
        "\n",
        "    # accuracy: (tp + tn) / (p + n)\n",
        "    accuracy = accuracy_score(Y_test_flattened, Y_predict_flattened)\n",
        "    print('Accuracy: %f' % accuracy)\n",
        "    # precision tp / (tp + fp)\n",
        "    precision = precision_score(Y_test_flattened, Y_predict_flattened)\n",
        "    print('Precision: %f' % precision)\n",
        "    # recall: tp / (tp + fn)\n",
        "    recall = recall_score(Y_test_flattened, Y_predict_flattened)\n",
        "    print('Recall: %f' % recall)\n",
        "    # f1: 2 tp / (2 tp + fp + fn)\n",
        "    f1 = f1_score(Y_test_flattened, Y_predict_flattened)\n",
        "    print('F1 score: %f' % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NIYTgYnuGGlC"
      },
      "source": [
        "Plots model's prediction for each `Ty` step for the `%index` example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "snTPi4pg9UpC",
        "colab": {}
      },
      "source": [
        "# visualises model's prediction on the %index example of training input X.\n",
        "def visualize_prediction(model, X, index=0):\n",
        "    Y_predict = model.predict(X, verbose=0)\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(Y_predict[index,:,0])\n",
        "    plt.ylabel('probability')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c_dH8fSwfyKh"
      },
      "source": [
        "Plots `Ty` for the `%index` example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IRRP11zlfo1y",
        "colab": {}
      },
      "source": [
        "def plot_Y(Y, index=0):\n",
        "    plt.plot(Y[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GEswPXMOF81",
        "colab": {}
      },
      "source": [
        "evaluate_model(model, X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YGdLi2c3JGnR",
        "colab": {}
      },
      "source": [
        "plot_Y(Y_test, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "viZmisCNI9bE",
        "colab": {}
      },
      "source": [
        "visualize_prediction(model, X_test, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZGwQ9AS6RRxQ"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpMIfZQRRRDM",
        "colab": {}
      },
      "source": [
        "model.save(PROJ_PATH + \"models/please_save_me.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pRMwSS97OLGr"
      },
      "source": [
        "## [Doesn't work] Confirming that the bias fix helps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JLoO6fdVOOzO",
        "colab": {}
      },
      "source": [
        "zero_bias_model = create_model(input_shape = (Tx, n_freq), output_bias = None)\n",
        "model_train(model, X, Y, X_dev, Y_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4fvrigkdOt8C",
        "colab": {}
      },
      "source": [
        "careful_bias_model = create_model(input_shape = (Tx, n_freq), output_bias = initial_bias)\n",
        "model_train(model, X, Y, X_dev, Y_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dywik-bMXMIH",
        "colab": {}
      },
      "source": [
        "evaluate_model(zero_bias_model, X_test, Y_test, 0.3)\n",
        "evaluate_model(careful_bias_model, X_test, Y_test, 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2u60EpQtdgPE",
        "colab": {}
      },
      "source": [
        "visualize_prediction(model, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jrLCu-jWdo-z",
        "colab": {}
      },
      "source": [
        "plt.plot(Y_test[0]) # TODO: add into evaluation tools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-zPszFjfdcoC",
        "colab": {}
      },
      "source": [
        "evaluate_model(model, X_test, Y_test, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJgwdZNKbYyF"
      },
      "source": [
        "Notice how there is only one sharp peak in the `careful_bias_model` but many sharp peaks in `zero_bias_model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s_aPp2kWaPl4",
        "colab": {}
      },
      "source": [
        "visualize_prediction(careful_bias_model, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVn3N1mhaTk2",
        "colab": {}
      },
      "source": [
        "visualize_prediction(zero_bias_model, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DV8XoPf_c7qx",
        "colab": {}
      },
      "source": [
        "plt.plot(Y_test[0]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Itc4LKmGZa5"
      },
      "source": [
        "[Can delete the code below since `model.evaluate` is redundant now; it does not use metrics we want (ie f score)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YVvPoblB6t-f",
        "colab": {}
      },
      "source": [
        "# For colab\n",
        "# Function to test the model on new data\n",
        "def model_test(model, X_dev, Y_dev):\n",
        "    loss, acc = model.evaluate(X_dev, Y_dev)\n",
        "    print(\"Dev set accuracy = \", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F6uTgE6187gq",
        "colab": {}
      },
      "source": [
        "model_test(model, X_dev, Y_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXPZrQlw_JDA",
        "colab": {}
      },
      "source": [
        "X_11 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_11X.npy\")\n",
        "Y_11 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_11Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_11.shape, Y_11.shape))\n",
        "\n",
        "X_12 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_12X.npy\")\n",
        "Y_12 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_12Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_12.shape, Y_12.shape))\n",
        "\n",
        "X_13 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_13X.npy\")\n",
        "Y_13 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_13Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_13.shape, Y_13.shape))\n",
        "\n",
        "X_14 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_14X.npy\")\n",
        "Y_14 = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_14Y.npy\")\n",
        "print(\"X: {} Y: {}\".format(X_14.shape, Y_14.shape))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RuZi1Ghj_I0S",
        "colab": {}
      },
      "source": [
        "model_test(model, X_11, Y_11)\n",
        "model_test(model, X_12, Y_12)\n",
        "model_test(model, X_13, Y_13)\n",
        "model_test(model, X_14, Y_14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lo6H0xFiu-Bq",
        "colab": {}
      },
      "source": [
        "list_of_dev_ids = [11,12,13,14,20,21,22,23,24,30,31,32,33,34,35,36,37,38,39,310,40,41,42,43,44]\n",
        "for id in list_of_dev_ids:\n",
        "    X_id = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_\" + str(id) + \"X.npy\")\n",
        "\n",
        "    Y_id = np.load(\"/content/drive/My Drive/filler_detection/train_data/dev_npy/cont_\" + str(id) + \"Y.npy\")\n",
        "    print((\"X\" + str(id) + \": {} Y\" + str(id) + \": {}\").format(X_id.shape, Y_id.shape))\n",
        "    model_test(model, X_id, Y_id)\n",
        "\n",
        "model_test(model, X_11, Y_11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IhT_wdWf8XKP"
      },
      "source": [
        "## Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "huv11AUU8XKQ"
      },
      "source": [
        "Redefine model train function to accept parameters..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1BEPqVS48XKQ",
        "colab": {}
      },
      "source": [
        "import talos\n",
        "\n",
        "# Function to fit and further train the model. Returns history (check documentation of `model.fit` for more details).\n",
        "# This function is a special version being used for hyperparameter optimization.\n",
        "def hyperparam_optimize(X, Y, X_dev, Y_dev, params):\n",
        "    \"\"\"\n",
        "    Function to train the model further using Adam optimiser and binary \n",
        "    cross entropy loss.\n",
        "    \n",
        "    Arguments:\n",
        "    model -- Model to train\n",
        "    X -- X data to train on\n",
        "    Y -- Y data to train on\n",
        "    \"\"\"\n",
        "    \n",
        "    model = create_model(input_shape = (Tx, n_freq), output_bias = None, dropout=params['dropout'])\n",
        "    \n",
        "    es = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                            min_delta=0.05, \n",
        "                                            patience=15, \n",
        "                                            verbose=1, \n",
        "                                            mode='auto', \n",
        "                                            baseline=None, \n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "    \n",
        "    opt = Adam(lr=params['lr'], beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "    model.compile(loss=f1_loss, optimizer=opt)\n",
        "    \n",
        "    history = model.fit(X, Y, batch_size=25, epochs=40, validation_data=(X_dev, Y_dev), callbacks = [es])\n",
        "    return history, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZhnlBXmX8XKU"
      },
      "source": [
        "Load params to use in Talos function..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Yp63TPJ8XKU",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'lr': [0.001, 0.003, 0.005, 0.007, 0.009, 0.01],\n",
        "    'dropout': [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KhF39wiu8XKX"
      },
      "source": [
        "Import Talos and use..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GSIkIwLN8XKY",
        "colab": {}
      },
      "source": [
        "import talos\n",
        "\n",
        "scan_object = talos.Scan(X, Y, \n",
        "                         x_val=X_dev, \n",
        "                         y_val=Y_dev, \n",
        "                         model=hyperparam_optimize, \n",
        "                         params=params, \n",
        "                         experiment_name='test')\n",
        "\n",
        "model_name = 'bryan_concat_LSTM_dropout_04'\n",
        "\n",
        "# Save object and model\n",
        "scan_object.best_model(metric='val_loss', asc=True).save(PROJ_PATH + f\"models/hyperparam_scan_data/{model_name}.h5\")\n",
        "with open(PROJ_PATH + f'models/hyperparam_scan_data/{model_name}.pkl', 'wb') as f:\n",
        "    pickle.dump(scan_object.data, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fSrp5i6p8XKe"
      },
      "source": [
        "Get Talos data after running hyperparameter optimization..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nBEk74A98XKg",
        "colab": {}
      },
      "source": [
        "# Get data\n",
        "with open(PROJ_PATH + f'models/hyperparam_scan_data/{model_name}.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}