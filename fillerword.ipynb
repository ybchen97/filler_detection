{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filler Word Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data synthesis: Creating a speech dataset\n",
    "### 1.1 - Listening to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "import wave\n",
    "import pylab\n",
    "from tf_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring Environment Variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_DIRECTORY = \"./raw_data/positive_data/\"\n",
    "BACKGROUND_DIRECTORY = \"./raw_data/background_data/\"\n",
    "NEGATIVES_DIRECTORY = \"./raw_data/google_dataset/\"\n",
    "NEGATIVES_TRUNCATED_DIRECTORY = \"./raw_data/google_dataset_truncated/\"\n",
    "AUDIO_EXAMPLES_DIRECTORY = \"./audio_examples/\"\n",
    "POSITIVE_EXAMPLE = \"jh_1.wav\"\n",
    "AUDIO_EXAMPLE = \"example_train.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading raw audio files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(AudioSegment.from_file(POSITIVE_DIRECTORY + POSITIVE_EXAMPLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - From audio recordings to spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(AudioSegment.from_file(AUDIO_EXAMPLES_DIRECTORY + AUDIO_EXAMPLE, format=\"wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate, samples = wavfile.read(AUDIO_EXAMPLES_DIRECTORY + AUDIO_EXAMPLE)\n",
    "frequencies, times, x = signal.spectrogram(samples, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time steps in input after spectrogram (129, 2142)\n"
     ]
    }
   ],
   "source": [
    "# _, data = wavfile.read(AUDIO_EXAMPLES_DIRECTORY + AUDIO_EXAMPLE)\n",
    "# print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "print(\"Time steps in input after spectrogram\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 2142 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 129 # Number of frequencies input to the model at each time step of the spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty = 1375 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Generating a single training example\n",
    "Note that 1.2 is skipped because we need to first have a training example before we can graph its spectogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw audio files for speech synthesis\n",
    "def load_raw_audio(positiveDirectory, backgroundDirectory, negativesDirectory):\n",
    "    BACKGROUND_DIRECTORY_IN_GOOGLE_DATASET = \"_background_noise_/\"\n",
    "    positives = []\n",
    "    backgrounds = []\n",
    "    negatives = []\n",
    "    for filename in os.listdir(positiveDirectory):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            activate = AudioSegment.from_wav(positiveDirectory + filename)\n",
    "            positives.append(activate)\n",
    "    for filename in os.listdir(backgroundDirectory):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            background = AudioSegment.from_wav(backgroundDirectory + filename)\n",
    "            backgrounds.append(background)\n",
    "    for directory in os.listdir(negativesDirectory):\n",
    "        if os.path.isdir(os.path.join(negativesDirectory, directory)) and directory != BACKGROUND_DIRECTORY_IN_GOOGLE_DATASET: # Excludes background directory in google dataset\n",
    "            for filename in os.listdir(negativesDirectory + directory):\n",
    "                if filename.endswith(\"wav\"):\n",
    "                    negative = AudioSegment.from_wav(negativesDirectory + directory + \"/\" + filename)\n",
    "                    negatives.append(negative)\n",
    "    return positives, negatives, backgrounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio segments using pydub \n",
    "# positives, negatives, backgrounds = load_raw_audio(POSITIVE_DIRECTORY, BACKGROUND_DIRECTORY, NEGATIVES_DIRECTORY)  # Actual dataset. Use this for production.\n",
    "positives, negatives, backgrounds = load_raw_audio(POSITIVE_DIRECTORY, BACKGROUND_DIRECTORY, NEGATIVES_TRUNCATED_DIRECTORY) # Truncated negative dataset. Use this to save time.\n",
    "\n",
    "assert len(backgrounds[0]) == 10000, \"Should be 10,000 since it is a 10s clip\"\n",
    "assert 0 < len(positives[0]) < 2000, \"Maybe around 1000, since a positive audio clip is usually around 1 sec (but varies a lot)\"\n",
    "assert 0 < len(negatives[0]) < 2000, \"Maybe around 1000, since a negative audio clip is usually around 1 sec (but varies a lot)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[OPTIONAL\\] Playing audio clips..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play(backgrounds[0])\n",
    "# play(positives[0])\n",
    "# play(negatives[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
    "    \n",
    "    Returns:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   # Make sure segment doesn't run past the 10sec background \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) for the new segment\n",
    "    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\n",
    "    \n",
    "    Returns:\n",
    "    True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start, segment_end = segment_time\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 line)\n",
    "    # Step 1: Initialize overlap as a \"False\" flag. (≈ 1 line)\n",
    "    overlap = False\n",
    "    \n",
    "    # Step 2: loop over the previous_segments start and end times.\n",
    "    # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)\n",
    "    for previous_start, previous_end in previous_segments:\n",
    "        if segment_start <= previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return overlap\n",
    "\n",
    "overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\n",
    "overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\n",
    "\n",
    "assert overlap1 == False, \"Should not overlap.\"\n",
    "assert overlap2 == True, \"Should overlap.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
    "    audio segment does not overlap with existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording.  \n",
    "    audio_clip -- the audio clip to be inserted/overlaid. \n",
    "    previous_segments -- times where audio segments have already been placed\n",
    "    \n",
    "    Returns:\n",
    "    new_background -- the updated background audio\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the duration of the audio clip in ms\n",
    "    segment_ms = len(audio_clip)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Step 1: Use one of the helper functions to pick a random time segment onto which to insert \n",
    "    # the new audio clip. (≈ 1 line)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    \n",
    "    # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep \n",
    "    # picking new segment_time at random until it doesn't overlap. (≈ 2 lines)\n",
    "    while is_overlapping(segment_time, previous_segments):\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "\n",
    "    # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)\n",
    "    previous_segments.append(segment_time)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Step 4: Superpose audio segment and background\n",
    "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    \n",
    "    return new_background, segment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clip, segment_time = insert_audio_clip(backgrounds[0], positives[0], [(3790, 4400)])\n",
    "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
    "print(\"Segment Time: \", segment_time)\n",
    "play(AudioSegment.from_file(\"insert_test.wav\", format=\"wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment \n",
    "    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n",
    "    50 followinf labels should be ones.\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    y -- numpy array of shape (1, Ty), the labels of the training example\n",
    "    segment_end_ms -- the end time of the segment in ms\n",
    "    \n",
    "    Returns:\n",
    "    y -- updated labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # duration of the background (in terms of spectrogram time-steps)\n",
    "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
    "    \n",
    "    # Add 1 to the correct index in the background label (y)\n",
    "    ### START CODE HERE ### (≈ 3 lines)\n",
    "    for i in range(segment_end_y + 1, segment_end_y + 51):\n",
    "        if i < Ty:\n",
    "            y[0, i] = 1\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = insert_ones(np.zeros((1, Ty)), 9700)\n",
    "plt.plot(insert_ones(arr1, 4251)[0,:])\n",
    "print(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(background, positives, negatives):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, positives, and negatives.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording\n",
    "    positives -- a list of audio segments of the word \"activate\"\n",
    "    negatives -- a list of audio segments of random words that are not \"activate\"\n",
    "    \n",
    "    Returns:\n",
    "    x -- the spectrogram of the training example\n",
    "    y -- the label at each time step of the spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make background quieter - commented out as background is already very soft\n",
    "    # background = background - 20\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Initialize y (label vector) of zeros (≈ 1 line)\n",
    "    y = np.zeros((1, Ty))\n",
    "\n",
    "    # Step 2: Initialize segment times as empty list (≈ 1 line)\n",
    "    previous_segments = []\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Select 0-4 random \"activate\" audio clips from the entire list of \"positives\" recordings\n",
    "    number_of_positives = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(positives), size=number_of_positives)\n",
    "    random_positives = [positives[i] for i in random_indices]\n",
    "    \n",
    "    # Step 3: Loop over randomly selected \"activate\" clips and insert in background\n",
    "    for random_activate in random_positives:\n",
    "        # Insert the audio clip on the background\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        # Retrieve segment_start and segment_end from segment_time\n",
    "        segment_start, segment_end = segment_time\n",
    "        # Insert labels in \"y\"\n",
    "        y = insert_ones(y, segment_end_ms=segment_end)\n",
    "\n",
    "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "\n",
    "    # Step 4: Loop over randomly selected negative clips and insert in background\n",
    "    for random_negative in random_negatives:\n",
    "        # Insert the audio clip on the background \n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "    \n",
    "    # Standardize the volume of the audio clip \n",
    "    # background = match_target_amplitude(background, -20.0)\n",
    "\n",
    "    # Export new training example \n",
    "    background = background.set_channels(1)\n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "    print(\"File (train.wav) was saved in your directory.\")\n",
    "\n",
    "    sample_rate, samples = wavfile.read(\"train.wav\")\n",
    "    frequencies, times, x = signal.spectrogram(samples, sample_rate)\n",
    "    \n",
    "    return frequencies, times, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies, times, x, y = create_training_example(backgrounds[0], positives, negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(AudioSegment.from_file(\"train.wav\", format=\"wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the x-axis is not in 10^-2 seconds. It's 10 seconds discretised to 1375 units. (read 1.2)\n",
    "plt.plot(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[Optional\\] Plotting spectrogram..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(times, frequencies, np.log(x))\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
